# DataPM - Pipeline Completo de An√°lisis del Mercado Laboral

## üéØ Descripci√≥n del Proyecto

DataPM es una suite completa de herramientas para la extracci√≥n, procesamiento, an√°lisis y visualizaci√≥n de ofertas de trabajo del mercado laboral. El sistema implementa un pipeline de extremo a extremo que va desde el web scraping hasta dashboards interactivos en tiempo real.

## üèóÔ∏è Arquitectura del Sistema

```
Web Scraping ‚Üí Limpieza ‚Üí Normalizaci√≥n ‚Üí Cloud Storage ‚Üí BigQuery ‚Üí Looker Studio
     ‚Üì             ‚Üì           ‚Üì              ‚Üì            ‚Üì           ‚Üì
  Raw Data    Deduplication  LLM Processing  Auto-Sync   Analytics  Dashboards
```

## üõ†Ô∏è Componentes del Pipeline

### 1. üï∑Ô∏è **Web Scraping Tools**
Herramientas especializadas para extraer ofertas de trabajo de websites de empleo.

#### **Scraper de Ofertas Abiertas** (`linkedin_selenium.py`)
- **Funci√≥n**: Extrae ofertas de trabajo disponibles p√∫blicamente
- **Caracter√≠sticas**:
  - B√∫squeda por keywords y ubicaci√≥n
  - Filtros de modalidad (remoto, h√≠brido, presencial)
  - Paginaci√≥n autom√°tica con anti-detecci√≥n
  - Extracci√≥n de t√≠tulo, empresa, ubicaci√≥n y descripci√≥n
- **Salida**: CSV en `csv/src/scrapped/`

#### **Scraper de Aplicaciones Personales** (`linkedin_applied_scraper.py`)
- **Funci√≥n**: Rastrea ofertas a las que ya se ha aplicado individualmente
- **Caracter√≠sticas**:
  - Login manual seguro (sin almacenar credenciales)
  - Soporte para autenticaci√≥n 2FA
  - Extracci√≥n de fechas de aplicaci√≥n
  - Tracking de estado de aplicaciones
- **Salida**: CSV en `csv/src/scrapped/Applied/`

#### **Pipeline de URL Individual** (`single_url_pipeline.py`)
- **Funci√≥n**: Procesa ofertas espec√≠ficas mediante URL directa
- **Caracter√≠sticas**:
  - Soporte multi-sitio (websites de empleo diversos)
  - Procesamiento autom√°tico con DataPM engine
  - Modo headless opcional
- **Uso**: `python single_url_pipeline.py --url [JOB_URL]`

### 2. üßπ **Sistema de Limpieza y Deduplicaci√≥n**
Herramientas avanzadas para limpiar y normalizar datos antes del procesamiento.

#### **Deduplicaci√≥n B√°sica** (`deduplication_processor.py`)
- **Funci√≥n**: Elimina duplicados exactos dentro de archivos individuales
- **Algoritmo**: Hash MD5 de todos los campos
- **Modos**: `latest` (archivo m√°s reciente) | `cross` (entre archivos) | `all` (legacy)

#### **Deduplicaci√≥n Avanzada** (`advanced_deduplication_processor.py`)
- **Funci√≥n**: Detecta duplicados similares usando an√°lisis sem√°ntico
- **Caracter√≠sticas**:
  - Similarity scoring con umbral configurable
  - An√°lisis de campos individuales vs completos
  - Reportes detallados de similitudes

#### **Deduplicaci√≥n Final** (`final_deduplication_processor.py`)
- **Funci√≥n**: Limpieza final cross-file de todo el dataset
- **Caracter√≠sticas**:
  - An√°lisis exhaustivo entre m√∫ltiples archivos
  - Preservaci√≥n de metadatos de origen
  - Reportes de calidad de datos

### 3. ü§ñ **Motor de Procesamiento con LLM**
Sistema de normalizaci√≥n inteligente usando modelos de lenguaje.

#### **Procesador Principal** (`datapm_processor.py`)
- **LLM**: Google Gemini 2.0 Flash
- **Funci√≥n**: Transforma descripciones de trabajo en datos estructurados
- **Caracter√≠sticas**:
  - Retry logic para rate limits
  - Procesamiento por batches (10 registros)
  - Esquemas de normalizaci√≥n extensivos
- **Salida**: CSV estructurado en `csv/src/csv_processed/`

#### **Procesador Ollama** (`datapm_processor_ollama.py`)
- **LLM**: Ollama (modelos locales)
- **Funci√≥n**: Alternativa local sin l√≠mites de API
- **Caracter√≠sticas**:
  - Modelos configurables (llama3.2:3b por defecto)
  - Fallback para respuestas no-JSON
  - Timeout extendido para procesamiento local

#### **Esquemas de Normalizaci√≥n** (`config.py`)
- **job_title_short**: 200+ t√≠tulos normalizados
- **skills**: 150+ habilidades categorizadas
- **software**: 100+ herramientas y tecnolog√≠as
- **seniority**: Junior, Mid, Senior, Lead, C-Level
- **experience_years**: Rangos estandarizados
- **degrees**: T√≠tulos universitarios requeridos

### 4. ‚òÅÔ∏è **Infraestructura en Google Cloud**

#### **Google Cloud Storage**
```
gs://datapm/
‚îú‚îÄ‚îÄ unified_processed/     # CSVs procesados para BigQuery
‚îî‚îÄ‚îÄ raw/                  # Archivos originales categorizados
```

#### **BigQuery - Modelo Dimensional**
- **Dataset**: `datapm-471620.csv_processed`
- **External Tables**: Auto-sync con GCS usando wildcards
- **Modelo Estrella**:
  - **Tabla de Hechos**: `job_offers_fact` (tabla central)
  - **Dimensiones**: `dim_companies`, `dim_skills`, `dim_software`
  - **Relaciones**: `dim_skill_job`, `dim_software_job`, `dim_degree_job`

### 5. üìä **Visualizaci√≥n y Analytics**

#### **Looker Studio Dashboard**
- **URL**: [Dashboard en Vivo](https://lookerstudio.google.com/reporting/b55a0154-496b-4c8e-89b7-2ee31318d0d4)
- **Actualizaci√≥n**: Tiempo real (auto-refresh habilitado)
- **An√°lisis Disponibles**:
  - Distribuci√≥n de roles m√°s demandados
  - An√°lisis de requisitos educativos (con/sin t√≠tulo)
  - Top skills y tecnolog√≠as solicitadas
  - An√°lisis geogr√°fico de ofertas
  - Distribuci√≥n por nivel de seniority
  - Correlaciones entre experiencia y requisitos

## üöÄ Flujo de Trabajo Completo

### **Flujo Principal**
```bash
# 1. Scraping de ofertas
python scrapper/linkedin_selenium.py --page-start 1 --page-end 5

# 2. Deduplicaci√≥n (recomendado: cross-file)
python csv_engine/engines/deduplication_processor.py --mode cross

# 3. Procesamiento con LLM
python csv_engine/engines/datapm_processor.py --input latest

# 4. Subida a Google Cloud Storage
gcloud storage cp csv/src/csv_processed/*.csv gs://datapm/unified_processed/

# 5. Visualizaci√≥n autom√°tica en Looker Studio
# (Los dashboards se actualizan autom√°ticamente)
```

### **Flujos Secundarios**

#### **Tracking de Aplicaciones Personales**
```bash
python scrapper/linkedin_applied_scraper.py
```

#### **Procesamiento de URL Individual**
```bash
python scrapper/single_url_pipeline.py --url "https://[JOB_URL]"
```

#### **Limpieza Avanzada de Datos**
```bash
python csv_engine/engines/final_deduplication_processor.py
```

## üìÅ Estructura del Proyecto

```
DataPM/
‚îú‚îÄ‚îÄ scrapper/                          # üï∑Ô∏è Herramientas de web scraping
‚îÇ   ‚îú‚îÄ‚îÄ linkedin_selenium.py          # Scraper de ofertas p√∫blicas
‚îÇ   ‚îú‚îÄ‚îÄ linkedin_applied_scraper.py   # Scraper de aplicaciones personales
‚îÇ   ‚îú‚îÄ‚îÄ single_url_pipeline.py        # Pipeline de URL individual
‚îÇ   ‚îî‚îÄ‚îÄ config.py                     # Configuraci√≥n de scrapers
‚îÇ
‚îú‚îÄ‚îÄ csv_engine/                        # ü§ñ Motor de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ engines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datapm_processor.py       # Procesador con Gemini
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datapm_processor_ollama.py # Procesador con Ollama
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deduplication_processor.py # Limpieza b√°sica
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_deduplication_processor.py # Limpieza avanzada
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ final_deduplication_processor.py # Limpieza final
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ config.py                 # Esquemas de normalizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ csv/src/                          # üìÅ Datos del pipeline
‚îÇ   ‚îú‚îÄ‚îÄ scrapped/                     # Datos raw del scraping
‚îÇ   ‚îú‚îÄ‚îÄ scrapped_deduplicated/        # Datos limpios
‚îÇ   ‚îú‚îÄ‚îÄ csv_processed/                # Datos procesados con LLM
‚îÇ   ‚îî‚îÄ‚îÄ csv_duplicates/               # Archivos de duplicados
‚îÇ
‚îú‚îÄ‚îÄ bigquery_setup_instructions.md    # üóÉÔ∏è Configuraci√≥n BigQuery
‚îú‚îÄ‚îÄ dimensional_model_bigquery.sql    # üìä Modelo dimensional SQL
‚îú‚îÄ‚îÄ bigquery_architecture.md          # üèóÔ∏è Documentaci√≥n t√©cnica
‚îî‚îÄ‚îÄ looker_studio_documentation.md    # üìà Especificaciones dashboards
```

## üîß Instalaci√≥n y Configuraci√≥n

### **Requisitos del Sistema**
```bash
Python 3.8+
Google Chrome + ChromeDriver (auto-gestionado)
Google Cloud SDK (para subida a GCS)
```

### **Dependencias Python**
```bash
pip install -r requirements.txt
```

**Principales librer√≠as**:
- `selenium` + `webdriver-manager` - Web scraping
- `google-generativeai` - Integraci√≥n con Gemini
- `requests` - HTTP requests para Ollama
- `pandas` - Manipulaci√≥n de datos
- `beautifulsoup4` - Parsing HTML

### **Configuraci√≥n de APIs**

#### **Google Gemini**
```python
# En csv_engine/utils/config.py
GEMINI_API_KEY = "tu_api_key_aqui"
GEMINI_MODEL = "gemini-2.0-flash-exp"
```

#### **Ollama (Alternativa Local)**
```bash
# Instalar Ollama
ollama pull llama3.2:3b

# Iniciar servidor
ollama serve
```

#### **Google Cloud**
```bash
# Autenticaci√≥n
gcloud auth login

# Configurar proyecto
gcloud config set project datapm-471620
```

## üìä M√©tricas y Resultados

### **Calidad de Datos**
- **Tasa de procesamiento exitoso**: >95%
- **Reducci√≥n de duplicados**: 15-25% en datasets t√≠picos
- **Campos normalizados correctamente**: >90%
- **Cobertura de skills identificadas**: >85%

### **Performance del Sistema**
- **Scraping**: 50-100 ofertas/minuto
- **Procesamiento LLM**: 10 registros/batch (2-3 min/batch)
- **Deduplicaci√≥n**: 1000+ registros/segundo
- **Actualizaci√≥n BigQuery**: <5 minutos

### **An√°lisis de Mercado Disponibles**
- **Roles m√°s demandados**: Top 20 posiciones
- **Skills cr√≠ticas**: Tecnolog√≠as m√°s solicitadas
- **Distribuci√≥n geogr√°fica**: Concentraci√≥n por ciudades
- **Requisitos educativos**: % trabajos con/sin t√≠tulo
- **Niveles de seniority**: Distribuci√≥n por experiencia

## üîó Enlaces Importantes

### **üåê Recursos en Vivo**
- **[Dashboard Principal](https://lookerstudio.google.com/reporting/b55a0154-496b-4c8e-89b7-2ee31318d0d4)** - Analytics en tiempo real
- **[Repositorio GitHub](https://github.com/pedrojosehv/DataPM)** - C√≥digo fuente completo

### **‚òÅÔ∏è Infraestructura Cloud**
- **BigQuery Dataset**: `datapm-471620.csv_processed`
- **Cloud Storage**: `gs://datapm/unified_processed/`
- **External Tables**: Auto-sync habilitado

## üöÄ Escalabilidad y Futuras Mejoras

### **Preparado para Crecimiento**
- **Procesamiento distribuido**: Batches configurables
- **Multi-sitio**: Soporte para m√∫ltiples websites de empleo
- **Schema evolution**: Adaptaci√≥n autom√°tica a nuevos campos
- **API integration**: Preparado para APIs de terceros

### **Roadmap T√©cnico**
- [ ] **An√°lisis temporal**: Trends de mercado por tiempo
- [ ] **Predicci√≥n salarial**: ML models para estimaci√≥n
- [ ] **Alertas inteligentes**: Notificaciones de cambios significativos
- [ ] **Mobile optimization**: Dashboards responsive
- [ ] **Advanced NLP**: An√°lisis de sentimiento en descripciones

---

## üìÑ Documentaci√≥n Adicional

- **[Arquitectura BigQuery](bigquery_architecture.md)** - Detalles t√©cnicos del modelo dimensional
- **[Configuraci√≥n Looker Studio](looker_studio_documentation.md)** - Especificaciones de dashboards
- **[Setup BigQuery](bigquery_setup_instructions.md)** - Gu√≠a paso a paso de configuraci√≥n

---

**Autor**: Pedro Jos√© Hern√°ndez  
**Fecha**: Septiembre 2024  
**Tecnolog√≠as**: Python, Selenium, Google Gemini, Ollama, Google Cloud Platform, BigQuery, Looker Studio  
**Dashboard Live**: [lookerstudio.google.com/reporting/b55a0154-496b-4c8e-89b7-2ee31318d0d4](https://lookerstudio.google.com/reporting/b55a0154-496b-4c8e-89b7-2ee31318d0d4)

**Status**: ‚úÖ Producci√≥n | üìä Analytics en Tiempo Real | üöÄ Escalable