# DataPM Processor - Versi√≥n Ollama

üöÄ **Versi√≥n local del procesador de descripciones de trabajo usando Ollama**

Esta versi√≥n del programa usa Ollama (LLM local) en lugar de Google Gemini, eliminando completamente los l√≠mites de rate limiting y permitiendo procesamiento ilimitado.

## üéØ **Ventajas de la versi√≥n Ollama:**

- ‚úÖ **Sin l√≠mites de rate limiting** - Procesa tantos registros como quieras
- ‚úÖ **Completamente local** - No necesitas API keys ni conexi√≥n a internet
- ‚úÖ **Mismo schema y prompts** - Compatibilidad total con la versi√≥n Gemini
- ‚úÖ **M√∫ltiples modelos** - Usa cualquier modelo disponible en Ollama
- ‚úÖ **Procesamiento m√°s r√°pido** - Sin pausas por rate limiting

## üìã **Requisitos:**

### 1. **Instalar Ollama**
```bash
# Windows (PowerShell)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. **Instalar dependencias Python**
```bash
pip install requests
```

### 3. **Descargar un modelo**
```bash
# Modelo recomendado (3B par√°metros, r√°pido)
ollama pull llama3.2:3b

# Modelo m√°s potente (7B par√°metros, m√°s lento)
ollama pull llama3.2:7b

# Modelo especializado en c√≥digo
ollama pull codellama:7b
```

## üöÄ **Uso:**

### **Uso b√°sico:**
```bash
python datapm_processor_ollama.py linkedin_jobs_detailed.csv
```

### **Especificar modelo:**
```bash
python datapm_processor_ollama.py linkedin_jobs_detailed.csv --model llama3.2:7b
```

### **Especificar URL de Ollama:**
```bash
python datapm_processor_ollama.py linkedin_jobs_detailed.csv --ollama-url http://localhost:11434
```

### **Especificar archivo de salida:**
```bash
python datapm_processor_ollama.py linkedin_jobs_detailed.csv --output mi_resultado.csv
```

## üìä **Modelos recomendados:**

| Modelo | Tama√±o | Velocidad | Calidad | Uso recomendado |
|--------|--------|-----------|---------|-----------------|
| `llama3.2:3b` | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Procesamiento r√°pido |
| `llama3.2:7b` | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Balance calidad/velocidad |
| `codellama:7b` | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | An√°lisis t√©cnico |
| `mistral:7b` | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | An√°lisis general |

## üîß **Configuraci√≥n:**

### **Par√°metros disponibles:**
- `--model`: Modelo de Ollama a usar (default: llama3.2:3b)
- `--ollama-url`: URL del servidor Ollama (default: http://localhost:11434)
- `--output`: Archivo de salida personalizado
- `--help`: Mostrar ayuda

### **Configuraci√≥n de Ollama:**
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama list

# Ver modelos disponibles
ollama list

# Ejecutar Ollama en segundo plano
ollama serve
```

## üìà **Rendimiento:**

### **Comparaci√≥n con Gemini:**
| Aspecto | Gemini | Ollama |
|---------|--------|--------|
| Rate Limiting | 10 req/min | Sin l√≠mites |
| Velocidad | ‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö° |
| Calidad | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Costo | Gratis/Paid | Gratis |
| Privacidad | Cloud | Local |

### **Tiempos estimados:**
- **50 registros**: ~2-3 minutos
- **100 registros**: ~4-6 minutos  
- **500 registros**: ~15-25 minutos

## üõ†Ô∏è **Soluci√≥n de problemas:**

### **Error de conexi√≥n:**
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# Verificar en otro terminal
curl http://localhost:11434/api/tags
```

### **Modelo no encontrado:**
```bash
# Descargar el modelo
ollama pull llama3.2:3b

# Ver modelos disponibles
ollama list
```

### **Memoria insuficiente:**
- Usar modelos m√°s peque√±os (3B en lugar de 7B)
- Cerrar otras aplicaciones
- Aumentar memoria virtual

## üìÅ **Archivos generados:**

El programa genera archivos CSV con el formato:
```
csv/archive/YYYYMMDD_HHMMSS_DataPM_Ollama_result.csv
```

### **Estructura del CSV:**
- `Job title (original)`: T√≠tulo original del trabajo
- `Job title (short)`: T√≠tulo normalizado
- `Company`: Nombre de la empresa
- `Country`, `State`, `City`: Ubicaci√≥n
- `Schedule type`: Tipo de horario
- `Experience years`: A√±os de experiencia
- `Seniority`: Nivel de seniority
- `Skills`: Habilidades identificadas
- `Degrees`: Grados acad√©micos
- `Software`: Software mencionado

## üîÑ **Migraci√≥n desde Gemini:**

Si ya tienes la versi√≥n Gemini funcionando:

1. **Instalar Ollama** (ver requisitos arriba)
2. **Descargar modelo**:
   ```bash
   ollama pull llama3.2:3b
   ```
3. **Cambiar comando**:
   ```bash
   # Antes (Gemini)
   python datapm_processor.py archivo.csv --llm gemini --api-key TU_API_KEY
   
   # Ahora (Ollama)
   python datapm_processor_ollama.py archivo.csv
   ```

## üéâ **¬°Listo para usar!**

Con Ollama, puedes procesar archivos de cualquier tama√±o sin preocuparte por l√≠mites de rate limiting. El programa mantiene la misma calidad de an√°lisis que la versi√≥n Gemini pero con procesamiento ilimitado.

