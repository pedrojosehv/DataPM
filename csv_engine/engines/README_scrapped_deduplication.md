# DataPM Scrapped Deduplication Processor

## ğŸš€ DescripciÃ³n

Este programa estÃ¡ optimizado para eliminar duplicados de archivos CSV en la carpeta `scrapped` antes de procesarlos con Gemini, evitando asÃ­ el gasto innecesario de tokens en registros duplicados.

## ğŸ¯ Objetivos

- **Eliminar duplicados antes del procesamiento con LLM**: Reduce costos al evitar procesar registros idÃ©nticos
- **ValidaciÃ³n completa de campos**: Compara TODOS los campos de cada fila para identificar duplicados exactos
- **Procesamiento del archivo mÃ¡s reciente**: Solo procesa el archivo CSV mÃ¡s reciente en la carpeta scrapped
- **OptimizaciÃ³n de recursos**: Ahorra tokens de Gemini al limpiar datos antes del procesamiento

## ğŸ“ Estructura de Directorios

```
D:\Work Work\Upwork\DataPM\
â”œâ”€â”€ csv/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ scrapped/           # ğŸ“¥ Archivos CSV originales del scraper
â”‚       â””â”€â”€ scrapped_deduplicated/  # ğŸ“¤ Archivos deduplicados (salida)
â””â”€â”€ csv_engine/
    â””â”€â”€ engines/
        â”œâ”€â”€ deduplication_processor.py  # ğŸ§¹ Programa principal
        â””â”€â”€ run_scrapped_deduplication.py  # ğŸš€ Script de ejecuciÃ³n
```

## ğŸ”§ Funcionamiento

### ValidaciÃ³n de Duplicados

El programa valida duplicados comparando **TODOS los campos** de cada fila:
- `title` (tÃ­tulo del trabajo)
- `company` (empresa)
- `location` (ubicaciÃ³n)
- `description` (descripciÃ³n completa)

Dos registros se consideran duplicados si **todos los campos son idÃ©nticos**.

### Procesamiento

1. **Identifica el archivo mÃ¡s reciente** en `csv/src/scrapped/`
2. **Carga y analiza** todos los registros
3. **Genera hash signatures** basadas en todos los campos
4. **Identifica duplicados exactos** mediante comparaciÃ³n de hashes
5. **Guarda archivo deduplicado** en `csv/src/scrapped_deduplicated/`
6. **Guarda archivo de duplicados** para revisiÃ³n (si existen)

## ğŸš€ Uso

### OpciÃ³n 1: Script AutomÃ¡tico (Recomendado)

```bash
python csv_engine/engines/run_scrapped_deduplication.py
```

### OpciÃ³n 2: EjecuciÃ³n Directa

```bash
# Procesar solo el archivo mÃ¡s reciente (default)
python csv_engine/engines/deduplication_processor.py --mode latest

# Procesar todos los archivos (modo legacy)
python csv_engine/engines/deduplication_processor.py --mode all

# Especificar directorio personalizado
python csv_engine/engines/deduplication_processor.py --scrapped-dir "D:/custom/path"
```

## ğŸ“Š Salida

### Archivos Generados

1. **`deduplicated_{timestamp}_{original_filename}.csv`**
   - Archivo limpio sin duplicados
   - Listo para procesamiento con DataPM processor

2. **`duplicates_{timestamp}_{original_filename}.csv`** (solo si hay duplicados)
   - Registros identificados como duplicados
   - Para revisiÃ³n manual si es necesario

### Reporte en Consola

```
ğŸ§¹ DataPM Scrapped Deduplication Processor
==================================================
ğŸ“‚ Working directory: D:/Work Work/Upwork/DataPM/csv/src/scrapped
ğŸ“ Processing latest file: 20250820_154912_linkedin_jobs.csv
ğŸ“… Modified: 2025-08-20 15:49:12
ğŸ“Š Original records: 50
ğŸ” Duplicate Analysis:
   Total records: 50
   Key fields used: title, company, location
   Duplicate combinations: 3
   Duplicate records: 8

âœ… Processing completed!
ğŸ“ Output file: D:/Work Work/Upwork/DataPM/csv/src/scrapped_deduplicated/deduplicated_20250820_154912_linkedin_jobs.csv
ğŸ“ Duplicates file: D:/Work Work/Upwork/DataPM/csv/src/scrapped_deduplicated/duplicates_20250820_154912_linkedin_jobs.csv
ğŸ“Š Results:
   Original records: 50
   Final records: 42
   Duplicates removed: 8
   Reduction: 16.00%
```

## ğŸ’¡ Beneficios

### Antes del Procesamiento con Gemini
- âœ… **Ahorro de tokens**: No procesar registros duplicados
- âœ… **Tiempo reducido**: Menos registros = procesamiento mÃ¡s rÃ¡pido
- âœ… **Mejor calidad**: Datos limpios desde el inicio

### ComparaciÃ³n de Costos

**Sin deduplicaciÃ³n:**
- 50 registros Ã— costo por registro = Costo Total

**Con deduplicaciÃ³n:**
- 42 registros Ãºnicos Ã— costo por registro = Costo Reducido
- **Ahorro: 16%** (en el ejemplo anterior)

## ğŸ”„ Flujo de Trabajo Recomendado

1. **Ejecutar scraper** â†’ Genera CSV en `scrapped/`
2. **Ejecutar deduplicaciÃ³n** â†’ Genera CSV limpio en `scrapped_deduplicated/`
3. **Ejecutar DataPM processor** â†’ Procesa solo registros Ãºnicos

## âš™ï¸ ConfiguraciÃ³n

### ParÃ¡metros por Defecto

```python
DEFAULT_SCRAPPED_DIR = "D:/Work Work/Upwork/DataPM/csv/src/scrapped"
DEFAULT_MODE = "latest"  # Solo archivo mÃ¡s reciente
```

### PersonalizaciÃ³n

Modifica estos valores en `deduplication_processor.py`:
- `scrapped_dir`: Directorio de entrada
- `output_dir`: Directorio de salida

## ğŸš¨ Consideraciones

- **Archivos grandes**: Para archivos muy grandes (>1000 registros), el procesamiento puede tardar varios minutos
- **Memoria**: AsegÃºrate de tener suficiente RAM para archivos grandes
- **Backup**: Los archivos originales en `scrapped/` no se modifican
- **Reversibilidad**: Puedes revisar los duplicados eliminados en el archivo `duplicates_*.csv`

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "No CSV files found"
- Verifica que existan archivos `.csv` en la carpeta `scrapped/`
- Revisa los permisos de lectura del directorio

### Error: "Permission denied"
- AsegÃºrate de tener permisos de escritura en `scrapped_deduplicated/`
- Cierra archivos CSV que puedan estar abiertos

### Advertencia: "No duplicates found"
- Es normal si el scraper ya genera datos Ãºnicos
- No afecta el funcionamiento del sistema

## ğŸ“ˆ MÃ©tricas de Rendimiento

El programa reporta automÃ¡ticamente:
- **Registros originales**: Total de registros en el archivo
- **Registros finales**: Registros Ãºnicos despuÃ©s de deduplicaciÃ³n
- **Duplicados eliminados**: NÃºmero absoluto de registros duplicados
- **Porcentaje de reducciÃ³n**: Eficiencia de la deduplicaciÃ³n

---

**ğŸ’¡ RecomendaciÃ³n**: Ejecuta este programa **antes** de procesar con DataPM para optimizar el uso de tokens de Gemini.

